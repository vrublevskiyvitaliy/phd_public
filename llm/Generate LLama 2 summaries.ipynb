{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb04aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install bitsandbytes==0.40.2 transformers==4.31.0 peft==0.4.0 accelerate==0.21.0 datasets trl==0.4.7 sentencepiece scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's recommended to run notebook like:\n",
    "# jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "# iopub_data_rate_limit is used to unlimit the io load from files, like pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b8d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config private\n",
    "\n",
    "huggingface_token = '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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45885e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports should be here\n",
    "from huggingface_hub.hf_api import HfFolder\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ae856f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config for model\n",
    "\n",
    "# Model name\n",
    "model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "# Llama 2 has the padding on the right side\n",
    "padding_side = 'right'\n",
    "\n",
    "# MIPS does not support bfloat16, so we need to use float16\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "# Use GPU if possible, otherwise CPU\n",
    "device_map = 'auto'\n",
    "\n",
    "# File name that is used to store all unique sentances in dataset\n",
    "file_name_all_sentances = 'all_sentances.pkl'\n",
    "\n",
    "# File name that is used to store all unique sentances in dataset\n",
    "file_name_summary_all_sentances = 'llama_2_summary_sentances.pkl'\n",
    "\n",
    "# Batch size for sentence processing\n",
    "llm_batch_size = 3\n",
    "\n",
    "# Number of sentances that LLM should return \n",
    "num_return_sequences = 2\n",
    "\n",
    "# Max length of sentances that LLM should return, in chars\n",
    "max_sequences_length=200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72817bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vrublevskyi/miniconda3/envs/llama/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a1a2d3d0c24c3f9472303f73b9f73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked to read about travel. Do you have any book recommendations?\n",
      "I'm glad you like to read about travel! Here are some book recommendations that might interest you:\n",
      "1. \"The Travel Book\" by Lonely Planet - This is a comprehensive guide to traveling, with information on destinations around the world, as well as tips on how to plan and prepare for your trips.\n",
      "2. \"The Art of Travel\" by Alain de Botton - This book explores the psychology of travel and the ways in which it can broaden our horizons and change our perspectives.\n",
      "3. \"On the Road\" by Jack Kerouac - This classic novel follows the adventures of a group of friends as they travel across America in the 1950s, and is a great read for anyone who loves to travel and explore new places.\n",
      "4. \"The Beach\" by Alex Gar\n"
     ]
    }
   ],
   "source": [
    "# Log in the HF to get access to the model (LLama 2)\n",
    "HfFolder.save_token(huggingface_token)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, padding_side=padding_side)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# Check if the model is working\n",
    "def check_model_is_working():\n",
    "    sequences = pipeline(\n",
    "        'I liked to read about travel. Do you have any book recommendations?',\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")\n",
    "\n",
    "check_model_is_working()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9adc48cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 10944 unique sentances.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# Some sentances are present more than a few times, it is more efficient to get the list \n",
    "# of all unique sentances.\n",
    "def get_all_sentances_from_dataset(dataset):\n",
    "    all_sentances = set()\n",
    "    \n",
    "    for part in ['train', 'validation', 'test']:\n",
    "      for elem in dataset[part]:\n",
    "        all_sentances.add(elem['sentence1'])\n",
    "        all_sentances.add(elem['sentence2'])\n",
    "    \n",
    "    return all_sentances\n",
    "\n",
    "all_sentances = get_all_sentances_from_dataset(dataset)\n",
    "\n",
    "print(f\"We have found {len(all_sentances)} unique sentances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc597308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We need to process 10944 sentances.\n"
     ]
    }
   ],
   "source": [
    "# Try to load the already processed sentances\n",
    "# It is usefull if processing was stopped and we want to resume from the \n",
    "# point when we stopped.\n",
    "def load_already_processed_sentances():\n",
    "    try:\n",
    "        with open(file_name_summary_all_sentances, 'rb') as file:\n",
    "            processed_sentances = pickle.load(file)\n",
    "    except:\n",
    "        processed_sentances = {}\n",
    "    return processed_sentances\n",
    "\n",
    "def save_already_processed_sentances(all_sentances_processed):\n",
    "    with open(file_name_summary_all_sentances, 'wb') as fp:\n",
    "        pickle.dump(all_sentances_processed, fp)\n",
    "\n",
    "already_processed_sentances = load_already_processed_sentances()  \n",
    "\n",
    "sentances_to_process = all_sentances.difference(set(already_processed_sentances.keys())) \n",
    "print(f\"We need to process {len(sentances_to_process)} sentances.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9556bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models are better working when a few inputs are present, but to improve reliability\n",
    "# let's split into chunks and save partial results\n",
    "\n",
    "sentances_to_process_list = list(sentances_to_process)\n",
    "\n",
    "def batch_sentances(sentances, chunk_size):\n",
    "     \n",
    "    result = []\n",
    "    for i in range(0, len(sentances), chunk_size):\n",
    "        result.append(sentances[i:i + chunk_size])\n",
    "    return result\n",
    "\n",
    "sentances_to_llm_batched = batch_sentances(sentances_to_process_list, llm_batch_size)\n",
    "    \n",
    "sentances_to_llm_sample = sentances_to_llm_batched[0]\n",
    "\n",
    "def process_prompt_batch(batch):\n",
    "    global already_processed_sentances\n",
    "    \n",
    "    prompts_to_llm = [f\"Provide a summary of this text: {s}?\\n Summary:\" for s in batch]\n",
    "    \n",
    "    sequences = pipeline(\n",
    "        prompts_to_llm,\n",
    "        top_k=10,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=max_sequences_length,\n",
    "        return_full_text = False,\n",
    "    )\n",
    "    for original_s, generated_batch in zip(batch, sequences):\n",
    "        already_processed_sentances[original_s] = [s['generated_text'] for s in generated_batch]\n",
    "    \n",
    "    save_already_processed_sentances(already_processed_sentances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75161683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch:\n",
      "['The 14-year-old national spelling finalist who attends school in Belmont , N.C. , got a word that sounded like \" zistee \" during competition Wednesday .', 'The euro was at 1.5281 versus the Swiss franc EURCHF = , up 0.2 percent on the session , after hitting its highest since mid-2001 around 1.5292 earlier in the session .', 'Prosecutors called Durst a cold-blooded killer who shot Black to steal his identity .']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample batch:\")\n",
    "print(sentances_to_llm_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d09fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_prompt_batch(sentances_to_llm_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "already_processed_sentances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama] *",
   "language": "python",
   "name": "conda-env-llama-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
