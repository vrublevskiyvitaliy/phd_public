{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ebd0444cbecb41afb393ef627e2264b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1f63ea47d5d43acaa1312f5d5e12ad6",
              "IPY_MODEL_ad66208133c042e1b59e5dfd10a2ac57",
              "IPY_MODEL_6d3fbd9bbb3a41779a45b2e36c6bf80a"
            ],
            "layout": "IPY_MODEL_a742602989f249a882eb66d3e33ba30e"
          }
        },
        "b1f63ea47d5d43acaa1312f5d5e12ad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b91bbeb0c5bd4a02ad6bd7cce8de9e43",
            "placeholder": "​",
            "style": "IPY_MODEL_9adea17bed5f46d68ee60e955e6cf710",
            "value": "Map: 100%"
          }
        },
        "ad66208133c042e1b59e5dfd10a2ac57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3db4f1a307f48b481fcd66689f2400f",
            "max": 3668,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30f4087352ef4bcd9b3b75769dfd0e66",
            "value": 3668
          }
        },
        "6d3fbd9bbb3a41779a45b2e36c6bf80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c511f75bf2049fa8a5fca54a7315441",
            "placeholder": "​",
            "style": "IPY_MODEL_688cbcc926c64059bf152a696a264b00",
            "value": " 3668/3668 [00:11&lt;00:00, 481.78 examples/s]"
          }
        },
        "a742602989f249a882eb66d3e33ba30e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b91bbeb0c5bd4a02ad6bd7cce8de9e43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9adea17bed5f46d68ee60e955e6cf710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3db4f1a307f48b481fcd66689f2400f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30f4087352ef4bcd9b3b75769dfd0e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7c511f75bf2049fa8a5fca54a7315441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "688cbcc926c64059bf152a696a264b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a1deb5c054c48d889b4305013481a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56ac8c0be67d455ea1973f765cc58abd",
              "IPY_MODEL_278627f37492431da0cf042d789360eb",
              "IPY_MODEL_899cd9f7b01f40fdaedbb1226ef04868"
            ],
            "layout": "IPY_MODEL_3f285d931ba649e3ba7d2c164230a165"
          }
        },
        "56ac8c0be67d455ea1973f765cc58abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2eb477e507e4015ac4b10591e29380f",
            "placeholder": "​",
            "style": "IPY_MODEL_8a936920ce3540299173f29b9d7d8f4e",
            "value": "Map: 100%"
          }
        },
        "278627f37492431da0cf042d789360eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b18b4b797db4e4da8f7849a6f0f35b9",
            "max": 408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99377d19c29b455bb46b927cbaa12f0f",
            "value": 408
          }
        },
        "899cd9f7b01f40fdaedbb1226ef04868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc77b33e5c4544bcb900111465fa9fd9",
            "placeholder": "​",
            "style": "IPY_MODEL_bb7a0faeea864c93b17d414a54f1f872",
            "value": " 408/408 [00:01&lt;00:00, 1018.23 examples/s]"
          }
        },
        "3f285d931ba649e3ba7d2c164230a165": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2eb477e507e4015ac4b10591e29380f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a936920ce3540299173f29b9d7d8f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b18b4b797db4e4da8f7849a6f0f35b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99377d19c29b455bb46b927cbaa12f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc77b33e5c4544bcb900111465fa9fd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb7a0faeea864c93b17d414a54f1f872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d7d20ea7b7a42df8be025927dee4d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fb62469e2c64c0ba749d713a17a8c9c",
              "IPY_MODEL_f97d55e78dd0436a9d552cd6c6e53dd2",
              "IPY_MODEL_25e0f61ecf1e4b6d8734d779527fc45a"
            ],
            "layout": "IPY_MODEL_53ff24871b8c4423803238e3bca1bc15"
          }
        },
        "8fb62469e2c64c0ba749d713a17a8c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9e3e599b7f144c29f29b1eb9382aba0",
            "placeholder": "​",
            "style": "IPY_MODEL_53af2a863e294b97921b2dcb226e0775",
            "value": "Map: 100%"
          }
        },
        "f97d55e78dd0436a9d552cd6c6e53dd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daccd22f0de8412594bcd0722469f0f3",
            "max": 1725,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ee5219eb94c4fcd9cb3a7e505e8974c",
            "value": 1725
          }
        },
        "25e0f61ecf1e4b6d8734d779527fc45a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99f9f7e36b3d444c8149011fc16761a1",
            "placeholder": "​",
            "style": "IPY_MODEL_7ee5b924b23243c4a5746f88103044e6",
            "value": " 1725/1725 [00:05&lt;00:00, 608.72 examples/s]"
          }
        },
        "53ff24871b8c4423803238e3bca1bc15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9e3e599b7f144c29f29b1eb9382aba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53af2a863e294b97921b2dcb226e0775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daccd22f0de8412594bcd0722469f0f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ee5219eb94c4fcd9cb3a7e505e8974c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99f9f7e36b3d444c8149011fc16761a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ee5b924b23243c4a5746f88103044e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5f65b81e7de4569a8cefd5e7757cd44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c939ee00bb1e494fa65fbe4a531160f1",
              "IPY_MODEL_9afe5f3b8f41451bae9313dafcea6def",
              "IPY_MODEL_2faafa4db800468a9d3c6d8a5b73ea3d"
            ],
            "layout": "IPY_MODEL_e709c868fd1744499afedd3e11ab8970"
          }
        },
        "c939ee00bb1e494fa65fbe4a531160f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6e786a4bb584b23bde97a1c9da70d16",
            "placeholder": "​",
            "style": "IPY_MODEL_531ecae32715415c9856f32b0713e020",
            "value": " 43%"
          }
        },
        "9afe5f3b8f41451bae9313dafcea6def": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efbf4203dc364acbbeb57940ee1a2f57",
            "max": 4590,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74036d21a6e54a1a88e59748cf2ba855",
            "value": 1988
          }
        },
        "2faafa4db800468a9d3c6d8a5b73ea3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be05ccc2ac04491ebacfa61e2e2cbcf9",
            "placeholder": "​",
            "style": "IPY_MODEL_411c4e162ced44779c44ae6599368d0e",
            "value": " 1988/4590 [36:21&lt;35:34,  1.22it/s]"
          }
        },
        "e709c868fd1744499afedd3e11ab8970": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6e786a4bb584b23bde97a1c9da70d16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "531ecae32715415c9856f32b0713e020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efbf4203dc364acbbeb57940ee1a2f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74036d21a6e54a1a88e59748cf2ba855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be05ccc2ac04491ebacfa61e2e2cbcf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "411c4e162ced44779c44ae6599368d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets transformers==4.28.0 evaluate"
      ],
      "metadata": {
        "id": "-TH4wY5rdS3k"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "haHsUHAjC__h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!rm -rf phd_public\n",
        "!git clone https://github.com/vrublevskiyvitaliy/phd_public.git"
      ],
      "metadata": {
        "id": "qCzAxB3ifdUR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import evaluate\n",
        "\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, get_scheduler, AutoConfig\n",
        "from transformers.utils import PaddingStrategy\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.tokenization_utils_base import TruncationStrategy\n",
        "\n",
        "from datasets import load_dataset, load_metric\n",
        "from functools import partial\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from torch.optim import AdamW\n"
      ],
      "metadata": {
        "id": "OYVqKwxwexpt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from phd_public.models.enriched_tokeniser import preprocess_dataset_final\n",
        "# from phd_public.models.deberta_model_attention_change import DebertaForSequenceClassificationV2\n",
        "from phd_public.utils.seed import init_seed\n",
        "# from phd_public.models.deberta_model_classic import DebertaForSequenceClassificationClassic\n",
        "# from phd_public.models.bert_tokeniser_with_pos_tags import preprocess_dataset_with_pos_tags\n",
        "\n"
      ],
      "metadata": {
        "id": "Hyq8f_sHfeCk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "634aca76-0473-4ccd-a5c0-f4ae81a435ea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GLOBALS\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MAX_LEN = 256\n",
        "BATCH_SIZE = 8\n",
        "TRUNCATION = TruncationStrategy.LONGEST_FIRST\n",
        "PADDING=PaddingStrategy.MAX_LENGTH\n",
        "SEED = 42\n",
        "LR = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 10\n",
        "# DATASET_PART = \"[:10%]\"\n",
        "DATASET_PART = \"\"\n",
        "\n",
        "\n",
        "model_name = \"microsoft/deberta-base\""
      ],
      "metadata": {
        "id": "daAHfOu2enGO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_seed(SEED)"
      ],
      "metadata": {
        "id": "7YRT0-98FMYK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "assert(transformers.__version__ == '4.28.0')\n",
        "\n",
        "from torch import nn\n",
        "from transformers.models.deberta.modeling_deberta import (\n",
        "    DebertaLayerNorm,\n",
        "    DebertaPreTrainedModel,\n",
        "    StableDropout,\n",
        "    DebertaEncoder,\n",
        "    ContextPooler,\n",
        "    DisentangledSelfAttention,\n",
        "    DebertaModel,\n",
        "    DebertaForSequenceClassification,\n",
        "    DebertaIntermediate,\n",
        "    XSoftmax,\n",
        "    DebertaOutput,\n",
        "    DebertaEmbeddings,\n",
        "    DebertaSelfOutput,\n",
        "    build_relative_position\n",
        ")\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from transformers.modeling_outputs import BaseModelOutput, SequenceClassifierOutput\n",
        "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss, MSELoss\n",
        "\n",
        "from collections.abc import Sequence\n",
        "\n",
        "class DisentangledSelfAttentionV2(DisentangledSelfAttention):\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask,\n",
        "        output_attentions=False,\n",
        "        query_states=None,\n",
        "        relative_pos=None,\n",
        "        rel_embeddings=None,\n",
        "        attention_enhencer=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Call the module\n",
        "\n",
        "        Args:\n",
        "            hidden_states (`torch.FloatTensor`):\n",
        "                Input states to the module usually the output from previous layer, it will be the Q,K and V in\n",
        "                *Attention(Q,K,V)*\n",
        "\n",
        "            attention_mask (`torch.ByteTensor`):\n",
        "                An attention mask matrix of shape [*B*, *N*, *N*] where *B* is the batch size, *N* is the maximum\n",
        "                sequence length in which element [i,j] = *1* means the *i* th token in the input can attend to the *j*\n",
        "                th token.\n",
        "\n",
        "            output_attentions (`bool`, optional):\n",
        "                Whether return the attention matrix.\n",
        "\n",
        "            query_states (`torch.FloatTensor`, optional):\n",
        "                The *Q* state in *Attention(Q,K,V)*.\n",
        "\n",
        "            relative_pos (`torch.LongTensor`):\n",
        "                The relative position encoding between the tokens in the sequence. It's of shape [*B*, *N*, *N*] with\n",
        "                values ranging in [*-max_relative_positions*, *max_relative_positions*].\n",
        "\n",
        "            rel_embeddings (`torch.FloatTensor`):\n",
        "                The embedding of relative distances. It's a tensor of shape [\\\\(2 \\\\times\n",
        "                \\\\text{max_relative_positions}\\\\), *hidden_size*].\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        if query_states is None:\n",
        "            qp = self.in_proj(hidden_states)  # .split(self.all_head_size, dim=-1)\n",
        "            query_layer, key_layer, value_layer = self.transpose_for_scores(qp).chunk(3, dim=-1)\n",
        "        else:\n",
        "\n",
        "            def linear(w, b, x):\n",
        "                if b is not None:\n",
        "                    return torch.matmul(x, w.t()) + b.t()\n",
        "                else:\n",
        "                    return torch.matmul(x, w.t())  # + b.t()\n",
        "\n",
        "            ws = self.in_proj.weight.chunk(self.num_attention_heads * 3, dim=0)\n",
        "            qkvw = [torch.cat([ws[i * 3 + k] for i in range(self.num_attention_heads)], dim=0) for k in range(3)]\n",
        "            qkvb = [None] * 3\n",
        "\n",
        "            q = linear(qkvw[0], qkvb[0], query_states.to(dtype=qkvw[0].dtype))\n",
        "            k, v = [linear(qkvw[i], qkvb[i], hidden_states.to(dtype=qkvw[i].dtype)) for i in range(1, 3)]\n",
        "            query_layer, key_layer, value_layer = [self.transpose_for_scores(x) for x in [q, k, v]]\n",
        "\n",
        "        query_layer = query_layer + self.transpose_for_scores(self.q_bias[None, None, :])\n",
        "        value_layer = value_layer + self.transpose_for_scores(self.v_bias[None, None, :])\n",
        "\n",
        "        rel_att = None\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        scale_factor = 1 + len(self.pos_att_type)\n",
        "        scale = torch.sqrt(torch.tensor(query_layer.size(-1), dtype=torch.float) * scale_factor)\n",
        "        query_layer = query_layer / scale.to(dtype=query_layer.dtype)\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        if self.relative_attention:\n",
        "            rel_embeddings = self.pos_dropout(rel_embeddings)\n",
        "            rel_att = self.disentangled_att_bias(query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\n",
        "\n",
        "        if rel_att is not None:\n",
        "            attention_scores = attention_scores + rel_att\n",
        "\n",
        "        ## APPLY HERE MODIFICATION VITALII TODO\n",
        "\n",
        "        # bxhxlxd\n",
        "        if self.talking_head:\n",
        "            attention_scores = self.head_logits_proj(attention_scores.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "        # attention_scores = Number of Batches x Num of heads x Max Length x Max Length\n",
        "        # attention_enhencer = Number of Batches x Num of heads x Max Length x Max Length\n",
        "        # attention_scores_ = attention_scores\n",
        "        if attention_enhencer is not None:\n",
        "          attention_scores = torch.mul(attention_scores, attention_enhencer)\n",
        "        # if not torch.equal(attention_scores_v2, attention_scores_):\n",
        "        #   print(\"Change in scores!\")\n",
        "        # else:\n",
        "        #   print(\"Scores are the same!\")\n",
        "        attention_probs = XSoftmax.apply(attention_scores, attention_mask, -1)\n",
        "\n",
        "        # attention_probs = Number of Batches x Num of heads x Max Length x Max Length\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        if self.talking_head:\n",
        "            attention_probs = self.head_weights_proj(attention_probs.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (-1,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "        if output_attentions:\n",
        "            return (context_layer, attention_probs)\n",
        "        else:\n",
        "            return context_layer\n",
        "\n",
        "\n",
        "class DebertaAttentionV2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self = DisentangledSelfAttentionV2(config)\n",
        "        self.output = DebertaSelfOutput(config)\n",
        "        self.config = config\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask,\n",
        "        attention_enhencer,\n",
        "        output_attentions=False,\n",
        "        query_states=None,\n",
        "        relative_pos=None,\n",
        "        rel_embeddings=None,\n",
        "    ):\n",
        "        self_output = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            output_attentions,\n",
        "            query_states=query_states,\n",
        "            relative_pos=relative_pos,\n",
        "            rel_embeddings=rel_embeddings,\n",
        "            attention_enhencer=attention_enhencer,\n",
        "        )\n",
        "        if output_attentions:\n",
        "            self_output, att_matrix = self_output\n",
        "        if query_states is None:\n",
        "            query_states = hidden_states\n",
        "        attention_output = self.output(self_output, query_states)\n",
        "\n",
        "        if output_attentions:\n",
        "            return (attention_output, att_matrix)\n",
        "        else:\n",
        "            return attention_output\n",
        "\n",
        "class DebertaLayerV2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = DebertaAttentionV2(config)\n",
        "        self.intermediate = DebertaIntermediate(config)\n",
        "        self.output = DebertaOutput(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask,\n",
        "        attention_enhencer,\n",
        "        query_states=None,\n",
        "        relative_pos=None,\n",
        "        rel_embeddings=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        attention_output = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            query_states=query_states,\n",
        "            relative_pos=relative_pos,\n",
        "            rel_embeddings=rel_embeddings,\n",
        "            attention_enhencer=attention_enhencer,\n",
        "        )\n",
        "        if output_attentions:\n",
        "            attention_output, att_matrix = attention_output\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        if output_attentions:\n",
        "            return (layer_output, att_matrix)\n",
        "        else:\n",
        "            return layer_output\n",
        "\n",
        "\n",
        "class DebertaEncoderV2(nn.Module):\n",
        "    \"\"\"Modified BertEncoder with relative position bias support\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.layer = nn.ModuleList([DebertaLayerV2(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.relative_attention = getattr(config, \"relative_attention\", False)\n",
        "        if self.relative_attention:\n",
        "            self.max_relative_positions = getattr(config, \"max_relative_positions\", -1)\n",
        "            if self.max_relative_positions < 1:\n",
        "                self.max_relative_positions = config.max_position_embeddings\n",
        "            self.rel_embeddings = nn.Embedding(self.max_relative_positions * 2, config.hidden_size)\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "    def get_rel_embedding(self):\n",
        "        rel_embeddings = self.rel_embeddings.weight if self.relative_attention else None\n",
        "        return rel_embeddings\n",
        "\n",
        "    def get_attention_mask(self, attention_mask):\n",
        "        if attention_mask.dim() <= 2:\n",
        "            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "            attention_mask = extended_attention_mask * extended_attention_mask.squeeze(-2).unsqueeze(-1)\n",
        "            attention_mask = attention_mask.byte()\n",
        "        elif attention_mask.dim() == 3:\n",
        "            attention_mask = attention_mask.unsqueeze(1)\n",
        "\n",
        "        return attention_mask\n",
        "\n",
        "    def get_rel_pos(self, hidden_states, query_states=None, relative_pos=None):\n",
        "        if self.relative_attention and relative_pos is None:\n",
        "            q = query_states.size(-2) if query_states is not None else hidden_states.size(-2)\n",
        "            relative_pos = build_relative_position(q, hidden_states.size(-2), hidden_states.device)\n",
        "        return relative_pos\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask,\n",
        "        attention_enhencer,\n",
        "        output_hidden_states=True,\n",
        "        output_attentions=False,\n",
        "        query_states=None,\n",
        "        relative_pos=None,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        attention_mask = self.get_attention_mask(attention_mask)\n",
        "        relative_pos = self.get_rel_pos(hidden_states, query_states, relative_pos)\n",
        "\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        if isinstance(hidden_states, Sequence):\n",
        "            next_kv = hidden_states[0]\n",
        "        else:\n",
        "            next_kv = hidden_states\n",
        "        rel_embeddings = self.get_rel_embedding()\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "                hidden_states = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    next_kv,\n",
        "                    attention_mask,\n",
        "                    query_states,\n",
        "                    relative_pos,\n",
        "                    rel_embeddings,\n",
        "                )\n",
        "            else:\n",
        "                hidden_states = layer_module(\n",
        "                    next_kv,\n",
        "                    attention_mask,\n",
        "                    attention_enhencer=attention_enhencer,\n",
        "                    query_states=query_states,\n",
        "                    relative_pos=relative_pos,\n",
        "                    rel_embeddings=rel_embeddings,\n",
        "                    output_attentions=output_attentions,\n",
        "                )\n",
        "\n",
        "            if output_attentions:\n",
        "                hidden_states, att_m = hidden_states\n",
        "\n",
        "            if query_states is not None:\n",
        "                query_states = hidden_states\n",
        "                if isinstance(hidden_states, Sequence):\n",
        "                    next_kv = hidden_states[i + 1] if i + 1 < len(self.layer) else None\n",
        "            else:\n",
        "                next_kv = hidden_states\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (att_m,)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n",
        "        )\n",
        "\n",
        "class DebertaModelV2(DebertaModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.embeddings = DebertaEmbeddings(config)\n",
        "        self.encoder = DebertaEncoderV2(config)\n",
        "        self.z_steps = 0\n",
        "        self.config = config\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        attention_enhencer: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, BaseModelOutput]:\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(input_shape, device=device)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask,\n",
        "            output_hidden_states=True,\n",
        "            output_attentions=output_attentions,\n",
        "            return_dict=return_dict,\n",
        "            attention_enhencer=attention_enhencer,\n",
        "        )\n",
        "        encoded_layers = encoder_outputs[1]\n",
        "\n",
        "        if self.z_steps > 1:\n",
        "            hidden_states = encoded_layers[-2]\n",
        "            layers = [self.encoder.layer[-1] for _ in range(self.z_steps)]\n",
        "            query_states = encoded_layers[-1]\n",
        "            rel_embeddings = self.encoder.get_rel_embedding()\n",
        "            attention_mask = self.encoder.get_attention_mask(attention_mask)\n",
        "            rel_pos = self.encoder.get_rel_pos(embedding_output)\n",
        "            for layer in layers[1:]:\n",
        "                query_states = layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    output_attentions=False,\n",
        "                    query_states=query_states,\n",
        "                    relative_pos=rel_pos,\n",
        "                    rel_embeddings=rel_embeddings,\n",
        "                    attention_enhencer=attention_enhencer,\n",
        "                )\n",
        "                encoded_layers.append(query_states)\n",
        "\n",
        "        sequence_output = encoded_layers[-1]\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output,) + encoder_outputs[(1 if output_hidden_states else 2) :]\n",
        "\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=sequence_output,\n",
        "            hidden_states=encoder_outputs.hidden_states if output_hidden_states else None,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "class DebertaForSequenceClassificationV2(DebertaForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        num_labels = getattr(config, \"num_labels\", 2)\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        self.deberta = DebertaModelV2(config)\n",
        "        self.pooler = ContextPooler(config)\n",
        "        output_dim = self.pooler.output_dim\n",
        "\n",
        "        self.classifier = nn.Linear(output_dim, num_labels)\n",
        "        drop_out = getattr(config, \"cls_dropout\", None)\n",
        "        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n",
        "        self.dropout = StableDropout(drop_out)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        attention_enhencer: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, SequenceClassifierOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
        "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
        "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.deberta(\n",
        "            input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            attention_enhencer=attention_enhencer,\n",
        "            position_ids=position_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        encoder_layer = outputs[0]\n",
        "        pooled_output = self.pooler(encoder_layer)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    # regression task\n",
        "\n",
        "                    loss_fn = nn.MSELoss()\n",
        "                    logits = logits.view(-1).to(labels.dtype)\n",
        "                    loss = loss_fn(logits, labels.view(-1))\n",
        "                elif labels.dim() == 1 or labels.size(-1) == 1:\n",
        "                    label_index = (labels >= 0).nonzero()\n",
        "                    labels = labels.long()\n",
        "                    if label_index.size(0) > 0:\n",
        "                        labeled_logits = torch.gather(\n",
        "                            logits, 0, label_index.expand(label_index.size(0), logits.size(1))\n",
        "                        )\n",
        "                        labels = torch.gather(labels, 0, label_index.view(-1))\n",
        "                        loss_fct = CrossEntropyLoss()\n",
        "                        loss = loss_fct(labeled_logits.view(-1, self.num_labels).float(), labels.view(-1))\n",
        "                    else:\n",
        "                        loss = torch.tensor(0).to(logits)\n",
        "                else:\n",
        "                    log_softmax = nn.LogSoftmax(-1)\n",
        "                    loss = -((log_softmax(logits) * labels).sum(-1)).mean()\n",
        "            elif self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                print(\"Here 3\")\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n",
        "        )\n"
      ],
      "metadata": {
        "id": "KGff5RBsXjJg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "model = DebertaForSequenceClassificationV2.from_pretrained(model_name,config=config)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VyrQhesdX7z",
        "outputId": "1e4763f6-9c34-4b63-fc90-e60e889f2f79"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassificationV2: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
            "- This IS expected if you are initializing DebertaForSequenceClassificationV2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForSequenceClassificationV2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaForSequenceClassificationV2 were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DebertaForSequenceClassificationV2(\n",
              "  (deberta): DebertaModelV2(\n",
              "    (embeddings): DebertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
              "      (LayerNorm): DebertaLayerNorm()\n",
              "      (dropout): StableDropout()\n",
              "    )\n",
              "    (encoder): DebertaEncoderV2(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x DebertaLayerV2(\n",
              "          (attention): DebertaAttentionV2(\n",
              "            (self): DisentangledSelfAttentionV2(\n",
              "              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): DebertaLayerNorm()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): DebertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): DebertaLayerNorm()\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (rel_embeddings): Embedding(1024, 768)\n",
              "    )\n",
              "  )\n",
              "  (pooler): ContextPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): StableDropout()\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): StableDropout()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "# Space module import\n",
        "import en_core_web_md\n",
        "from enum import Enum\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def get_spacy_module():\n",
        "  return en_core_web_md.load()\n",
        "\n",
        "nlp = get_spacy_module()\n",
        "\n",
        "def get_sentence_tokens(s, display = False):\n",
        "  doc = nlp(s)\n",
        "  sentence_tokens = []\n",
        "  if display:\n",
        "    spacy.displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "  for token in doc:\n",
        "    data = {\n",
        "      \"token_id\": token.i,\n",
        "      \"token_text\": token.text,\n",
        "      \"token_connection_ids\": token.head.i,\n",
        "      \"token_left_edge\": token.idx,\n",
        "      \"token_right_edge\": token.idx + len(token.text),\n",
        "      \"token_boundaries\": (token.idx, token.idx + len(token.text)),\n",
        "      \"token_pos_tag\": token.tag_,\n",
        "    }\n",
        "\n",
        "    sentence_tokens.append(data)\n",
        "  return sentence_tokens\n",
        "\n",
        "class TokeniserType(Enum):\n",
        "    ONE_SENTENCE = 1\n",
        "    TWO_SENTENCES = 2\n",
        "\n",
        "class BaseEnrichedTokeniser:\n",
        "  def __init__(self, tokeniser):\n",
        "    self._tokeniser = tokeniser\n",
        "    self.feature_key = None\n",
        "    self.type = TokeniserType.ONE_SENTENCE\n",
        "\n",
        "  def combine_transformer_and_sentence_features(self, transformer_tokens, sentence_features):\n",
        "    for i in range(len(transformer_tokens)):\n",
        "      transformer_tokens[i][self.feature_key] = None\n",
        "      for j in range(len(sentence_features)):\n",
        "        t_boundaries = transformer_tokens[i]['boundaries']\n",
        "        s_boundaries = sentence_features[j]['boundaries']\n",
        "        left = max(t_boundaries[0], s_boundaries[0])\n",
        "        right = min(t_boundaries[1], s_boundaries[1])\n",
        "        if left < right:\n",
        "          transformer_tokens[i][self.feature_key] = sentence_features[j][self.feature_key]\n",
        "\n",
        "      assert(transformer_tokens[i][self.feature_key] is not None)\n",
        "\n",
        "    return transformer_tokens\n",
        "\n",
        "  def get_feature(self, s):\n",
        "    raise Exception(\"Not implemented\")\n",
        "\n",
        "  def enrich_tokens(self, s):\n",
        "    t_tokens = self.get_transformer_sentence_tokens(s)\n",
        "    feature = self.get_feature(s)\n",
        "    combined_features = self.combine_transformer_and_sentence_features(t_tokens, feature)\n",
        "    return combined_features\n",
        "\n",
        "  \"\"\"\n",
        "  Return list of tokens that will be used in model\n",
        "  [('boundaries':(int, int), 'input_id':int, 'index':int]\n",
        "  \"\"\"\n",
        "  def get_transformer_sentence_tokens(self, s, verbose = False):\n",
        "      encoded = self._tokeniser.batch_encode_plus([s], return_offsets_mapping=True, add_special_tokens=False)\n",
        "      split_tokens = []\n",
        "      for i in range(len(encoded['input_ids'][0])):\n",
        "        split_tokens.append(\n",
        "            {\n",
        "                'index': i,\n",
        "                'input_id': encoded['input_ids'][0][i],\n",
        "                'boundaries': encoded['offset_mapping'][0][i],\n",
        "            }\n",
        "        )\n",
        "      return split_tokens\n",
        "\n",
        "  def post_processing(self, v):\n",
        "    return v\n",
        "\n",
        "class PosTagEnrichedTokeniser(BaseEnrichedTokeniser):\n",
        "\n",
        "  def __init__(self, tokeniser):\n",
        "    super().__init__(tokeniser)\n",
        "    self.feature_key = 'pos_tag'\n",
        "\n",
        "  def get_feature(self, s):\n",
        "    \"\"\"\n",
        "    Return list of [('boundaries':(int, int), 'feature':string)]\n",
        "    \"\"\"\n",
        "    all_tokens = get_sentence_tokens(s)\n",
        "    return [{'boundaries':token['token_boundaries'], 'pos_tag': token['token_pos_tag']} for token in all_tokens]\n",
        "\n",
        "\n",
        "class PosTagIdEnrichedTokeniser(BaseEnrichedTokeniser):\n",
        "  def __init__(self, tokeniser):\n",
        "    super().__init__(tokeniser)\n",
        "    self.feature_key = 'pos_tag_ids'\n",
        "    self.pos_tag_to_id_map = PosTagIdEnrichedTokeniser.load_pos_tag_value_to_idx()\n",
        "\n",
        "  @staticmethod\n",
        "  def load_pos_tag_value_to_idx():\n",
        "  # Computed based on test part of MRPC dataset.\n",
        "    return {\n",
        "        '$': 1,\n",
        "        \"''\": 2,\n",
        "        '(': 3,\n",
        "        ')': 4,\n",
        "        ',': 5,\n",
        "        '.': 6,\n",
        "        ':': 7,\n",
        "        'CC': 8,\n",
        "        'CD': 9,\n",
        "        'DT': 10,\n",
        "        'EX': 11,\n",
        "        'FW': 12,\n",
        "        'IN': 13,\n",
        "        'JJ': 14,\n",
        "        'JJR': 15,\n",
        "        'JJS': 16,\n",
        "        'LS': 17,\n",
        "        'MD': 18,\n",
        "        'NA': 19,\n",
        "        'NN': 20,\n",
        "        'NNP': 21,\n",
        "        'NNPS': 22,\n",
        "        'NNS': 23,\n",
        "        'PRP': 24,\n",
        "        'PRP$': 25,\n",
        "        'RB': 26,\n",
        "        'RBR': 27,\n",
        "        'SYM': 28,\n",
        "        'TO': 29,\n",
        "        'UH': 30,\n",
        "        'UNKNOWN': 31,\n",
        "        'VB': 32,\n",
        "        'VBD': 33,\n",
        "        'VBG': 34,\n",
        "        'VBN': 35,\n",
        "        'VBP': 36,\n",
        "        'VBZ': 37,\n",
        "        'WDT': 38,\n",
        "        'WP': 39,\n",
        "        'WP$': 40,\n",
        "        'WRB': 41,\n",
        "        '``': 42\n",
        "      }\n",
        "\n",
        "  def get_feature(self, s):\n",
        "    \"\"\"\n",
        "    Return list of [('boundaries':(int, int), 'feature':string)]\n",
        "    \"\"\"\n",
        "    all_tokens = get_sentence_tokens(s)\n",
        "    def get_id(pos_tag):\n",
        "      if pos_tag in self.pos_tag_to_id_map:\n",
        "        return self.pos_tag_to_id_map[pos_tag]\n",
        "      return self.pos_tag_to_id_map['UNKNOWN']\n",
        "    return [{'boundaries':token['token_boundaries'], self.feature_key: get_id(token['token_pos_tag'])} for token in all_tokens]\n",
        "\n",
        "  def post_processing(self, s1_s2_feature):\n",
        "    token_mapping = self.get_special_token_mapping()\n",
        "    def replace_token_id(token_id, m):\n",
        "      if token_id in m.keys():\n",
        "        return m[token_id]\n",
        "      return token_id\n",
        "    _s1_s2_feature = [replace_token_id(x, token_mapping) for x in s1_s2_feature]\n",
        "    return _s1_s2_feature\n",
        "\n",
        "  def get_special_token_mapping(self):\n",
        "    return {\n",
        "      self._tokeniser.cls_token_id: self.pos_tag_to_id_map['NA'],\n",
        "      self._tokeniser.sep_token_id: self.pos_tag_to_id_map['NA'],\n",
        "    }\n",
        "\n",
        "\n",
        "class AttentionEnhencerDummyEnrichedTokeniser(BaseEnrichedTokeniser):\n",
        "  def __init__(self, tokeniser):\n",
        "    super().__init__(tokeniser)\n",
        "    self.feature_key = 'attention_enhencer_dummy'\n",
        "    self.type = TokeniserType.TWO_SENTENCES\n",
        "\n",
        "  def enrich_tokens(self, s1, s2,  padding, truncation, max_length, _config):\n",
        "    # Dummy matrix will have 1 for all non padding elements.\n",
        "\n",
        "    dummy = self._tokeniser(s1, s2, truncation=truncation, max_length=max_length, padding=padding)\n",
        "\n",
        "    first_padding_0 = dummy['input_ids'].index(self._tokeniser.pad_token_id)\n",
        "    source = torch.full((first_padding_0,first_padding_0), 1.)\n",
        "    pad_distance =  max_length - first_padding_0\n",
        "\n",
        "    result = F.pad(input=source, pad=(0, pad_distance, 0, pad_distance), mode='constant', value=0.)\n",
        "    return result\n",
        "\n",
        "  def get_feature(self, s):\n",
        "    all_tokens = get_sentence_tokens(s)\n",
        "    return [\n",
        "      {\n",
        "        'boundaries':token['token_boundaries'],\n",
        "        'token_connection_ids': token['token_connection_ids'],\n",
        "        'token_id': token['token_id']\n",
        "      } for token in all_tokens\n",
        "    ]\n",
        "\n",
        "\n",
        "class AttentionEnhencerRandEnrichedTokenise(BaseEnrichedTokeniser):\n",
        "  def __init__(self, tokeniser):\n",
        "    super().__init__(tokeniser)\n",
        "    self.feature_key = 'attention_enhencer_rand'\n",
        "    self.type = TokeniserType.TWO_SENTENCES\n",
        "\n",
        "  def enrich_tokens(self, s1, s2,  padding, truncation, max_length, _config):\n",
        "    # Dummy matrix will have 1 for all non padding elements.\n",
        "\n",
        "    dummy = self._tokeniser(s1, s2, truncation=truncation, max_length=max_length, padding=padding)\n",
        "\n",
        "    first_padding_0 = dummy['input_ids'].index(self._tokeniser.pad_token_id)\n",
        "    source = torch.rand((first_padding_0,first_padding_0))\n",
        "    pad_distance =  max_length - first_padding_0\n",
        "\n",
        "    result = F.pad(input=source, pad=(0, pad_distance, 0, pad_distance), mode='constant', value=0.)\n",
        "    return result\n",
        "\n",
        "  def get_feature(self, s):\n",
        "    all_tokens = get_sentence_tokens(s)\n",
        "    return [\n",
        "      {\n",
        "        'boundaries':token['token_boundaries'],\n",
        "        'token_connection_ids': token['token_connection_ids'],\n",
        "        'token_id': token['token_id']\n",
        "      } for token in all_tokens\n",
        "    ]\n",
        "\n",
        "class AttentionEnhencerOneEnrichedTokenise(BaseEnrichedTokeniser):\n",
        "  def __init__(self, tokeniser):\n",
        "    super().__init__(tokeniser)\n",
        "    self.feature_key = 'attention_enhencer'\n",
        "    self.type = TokeniserType.TWO_SENTENCES\n",
        "\n",
        "  def enrich_tokens(self, s1, s2,  padding, truncation, max_length, _config):\n",
        "    # Dummy matrix will have 1 for all elements.\n",
        "    source = torch.full((max_length,max_length), 1.)\n",
        "\n",
        "    # number_of_attention_heads = 12\n",
        "    # source = source[None, :, :].expand([number_of_attention_heads, max_length, max_length])\n",
        "\n",
        "    return source\n",
        "\n",
        "class AttentionEnhencerDependancyTreeEnrichedTokenise(BaseEnrichedTokeniser):\n",
        "  def __init__(self, tokeniser):\n",
        "    super().__init__(tokeniser)\n",
        "    self.feature_key = 'attention_enhencer'\n",
        "    self.type = TokeniserType.TWO_SENTENCES\n",
        "\n",
        "  def enrich_tokens(self, s1, s2,  padding, truncation, max_length, config):\n",
        "    if 'att_dep_tree_pad_value' in config:\n",
        "      pad_value = config['att_dep_tree_pad_value']\n",
        "    else:\n",
        "      pad_value = 0.\n",
        "\n",
        "    data = self._tokeniser(s1, s2, truncation=truncation, max_length=max_length, padding=padding)\n",
        "    first_padding_0 = data['input_ids'].index(self._tokeniser.pad_token_id)\n",
        "    source = torch.full((first_padding_0,first_padding_0), 1.)\n",
        "    pad_distance =  max_length - first_padding_0\n",
        "    base_table = F.pad(input=source, pad=(0, pad_distance, 0, pad_distance), mode='constant', value=pad_value)\n",
        "    # Here we have table\n",
        "    # [1, ...1, 0, ..0]\n",
        "    # [...............]\n",
        "    # [1, ...1, 0, ..0]\n",
        "    # [...............]\n",
        "    # [0, ...0, 0, ..0]\n",
        "\n",
        "    # First sentence start\n",
        "    s1_start_inx = 1\n",
        "    sep_inx = data['input_ids'].index(self._tokeniser.sep_token_id)\n",
        "    s1_end_inx = sep_inx - 1\n",
        "    s2_start_inx = sep_inx + 1\n",
        "    s2_end_inx = first_padding_0 - 2\n",
        "\n",
        "    # print(f\"First start {s1_start_inx} end {s1_end_inx}\")\n",
        "    # print(f\"Second start {s2_start_inx} end {s2_end_inx}\")\n",
        "\n",
        "    # for i in range(s1_end_inx - s1_start_inx + 1):\n",
        "    #   for j in range(s1_end_inx - s1_start_inx + 1):\n",
        "    #     base_table[s1_start_inx + i][s1_start_inx + j] = 2.\n",
        "\n",
        "    # for i in range(s2_end_inx - s2_start_inx + 1):\n",
        "    #   for j in range(s2_end_inx - s2_start_inx + 1):\n",
        "    #     base_table[s2_start_inx + i][s2_start_inx + j] = 3.\n",
        "\n",
        "    s1_tokens = self.get_transformer_sentence_tokens(s1)\n",
        "    s2_tokens = self.get_transformer_sentence_tokens(s2)\n",
        "\n",
        "    s1_feature = self.get_feature(s1)\n",
        "    s2_feature = self.get_feature(s2)\n",
        "\n",
        "    def boundaries_match(transformer_token_boundary, sentence_feature_boundary):\n",
        "      left = max(transformer_token_boundary[0], sentence_feature_boundary[0])\n",
        "      right = min(transformer_token_boundary[1], sentence_feature_boundary[1])\n",
        "      return left < right\n",
        "\n",
        "    def build_token_map(tokens, features):\n",
        "      # Feature token id => [list of token ids]\n",
        "      m = {}\n",
        "      for i in range(len(features)):\n",
        "        m[i] = []\n",
        "        for j in range(len(tokens)):\n",
        "          feature_boundary = features[i]['boundaries']\n",
        "          token_boundary = tokens[j]['boundaries']\n",
        "          if boundaries_match(token_boundary, feature_boundary):\n",
        "            # we need to add pair\n",
        "            m[i].append(j)\n",
        "      return m\n",
        "\n",
        "    val = config['att_dep_tree_value']\n",
        "\n",
        "    def update_base_table(table, tokens, features, max_len, offset):\n",
        "      token_map = build_token_map(tokens, features)\n",
        "      for i in range(len(features)):\n",
        "        feature = features[i]\n",
        "        connected_node_id = feature['token_connection_ids']\n",
        "        feature_connected = features[connected_node_id]\n",
        "        tokens = token_map[i]\n",
        "        connected_tokens = token_map[connected_node_id]\n",
        "        for _x in tokens:\n",
        "          for _y in connected_tokens:\n",
        "            if _x < max_len and _y < max_len:\n",
        "              table[offset + _x][offset + _y] = val\n",
        "              table[offset + _y][offset + _x] = val\n",
        "      return table\n",
        "\n",
        "    base_table = update_base_table(base_table, s1_tokens, s1_feature, s1_end_inx - s1_start_inx + 1, s1_start_inx)\n",
        "    base_table = update_base_table(base_table, s2_tokens, s2_feature, s2_end_inx - s2_start_inx + 1, s2_start_inx)\n",
        "\n",
        "    # number_of_attention_heads = 12\n",
        "    # base_table = base_table[None, :, :].expand([number_of_attention_heads, max_length, max_length])\n",
        "    return base_table\n",
        "\n",
        "  def get_feature(self, s):\n",
        "    all_tokens = get_sentence_tokens(s)\n",
        "    return [\n",
        "      {\n",
        "        'boundaries':token['token_boundaries'],\n",
        "        'token_connection_ids': token['token_connection_ids'],\n",
        "        'token_id': token['token_id']\n",
        "      } for token in all_tokens\n",
        "    ]\n",
        "\n",
        "class FinalTokeniser:\n",
        "  def __init__(self, tokeniser, config):\n",
        "    self._tokeniser = tokeniser\n",
        "    self._config = config\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def _prepare_for_model(tokeniser, s1, s2, padding, truncation, max_length):\n",
        "    return tokeniser.prepare_for_model(\n",
        "      s1,\n",
        "      s2,\n",
        "      padding=padding,\n",
        "      truncation=truncation,\n",
        "      max_length=max_length,\n",
        "    )['input_ids']\n",
        "\n",
        "  def apply_tokenisers(self, s1, s2, tokeniser_list, padding, truncation, max_length):\n",
        "    # Get base data first: input ids, token_ids, attention_mask\n",
        "    encoded_base = self._tokeniser.batch_encode_plus([s1, s2], return_offsets_mapping=True, add_special_tokens=False)\n",
        "    s1_input_ids = encoded_base['input_ids'][0]\n",
        "    s2_input_ids = encoded_base['input_ids'][1]\n",
        "\n",
        "    _prepare_for_model = FinalTokeniser._prepare_for_model\n",
        "\n",
        "    s1_s2_input_ids = _prepare_for_model(self._tokeniser, s1_input_ids, s2_input_ids, padding, truncation, max_length)\n",
        "\n",
        "    data = {\n",
        "      'input_ids': s1_s2_input_ids,\n",
        "    }\n",
        "\n",
        "    for tokeniser in tokeniser_list:\n",
        "      if tokeniser.type == TokeniserType.ONE_SENTENCE:\n",
        "        enriched_data_s1 = tokeniser.enrich_tokens(s1)\n",
        "        enriched_data_s2 = tokeniser.enrich_tokens(s2)\n",
        "\n",
        "        _s1_input_ids = [x['input_id'] for x in enriched_data_s1]\n",
        "        _s2_input_ids = [x['input_id'] for x in enriched_data_s2]\n",
        "\n",
        "        assert(s1_input_ids == _s1_input_ids)\n",
        "        assert(s2_input_ids == _s2_input_ids)\n",
        "\n",
        "        s1_feature = [x[tokeniser.feature_key] for x in enriched_data_s1]\n",
        "        s2_feature = [x[tokeniser.feature_key] for x in enriched_data_s2]\n",
        "\n",
        "        s1_s2_feature = _prepare_for_model(self._tokeniser, s1_feature, s2_feature,  padding, truncation, max_length)\n",
        "      else:\n",
        "        s1_s2_feature = tokeniser.enrich_tokens(s1, s2,  padding, truncation, max_length, self._config)\n",
        "\n",
        "      s1_s2_feature = tokeniser.post_processing(s1_s2_feature)\n",
        "\n",
        "      data[tokeniser.feature_key] = s1_s2_feature\n",
        "\n",
        "\n",
        "    return data\n",
        "\n",
        "  def tokenise_everything(self, s1, s2, padding, truncation, max_length):\n",
        "    if 'tokeniser_list' in self._config:\n",
        "      tokeniser_list = self._config['tokeniser_list']\n",
        "    else:\n",
        "      tokeniser_list = ['dep']\n",
        "\n",
        "    list_of_tokenisers = []\n",
        "    if 'pos' in tokeniser_list:\n",
        "      pos_tag_id_tokeniser = PosTagIdEnrichedTokeniser(self._tokeniser)\n",
        "      list_of_tokenisers.append(pos_tag_id_tokeniser)\n",
        "\n",
        "    if 'attention_dummy' in tokeniser_list:\n",
        "      attention_dummy_tokeniser = AttentionEnhencerDummyEnrichedTokeniser(self._tokeniser)\n",
        "      list_of_tokenisers.append(attention_dummy_tokeniser)\n",
        "\n",
        "    if 'attention_dep' in tokeniser_list:\n",
        "      attention_tokeniser_dep = AttentionEnhencerDependancyTreeEnrichedTokenise(self._tokeniser)\n",
        "      list_of_tokenisers.append(attention_tokeniser_dep)\n",
        "\n",
        "    if 'attention_one' in tokeniser_list:\n",
        "      attention_tokeniser_one = AttentionEnhencerOneEnrichedTokenise(self._tokeniser)\n",
        "      list_of_tokenisers.append(attention_tokeniser_one)\n",
        "\n",
        "    AttentionEnhencerDependancyTreeEnrichedTokenise\n",
        "    return self.apply_tokenisers(\n",
        "      s1,\n",
        "      s2,\n",
        "      list_of_tokenisers,\n",
        "      padding,\n",
        "      truncation,\n",
        "      max_length\n",
        "    )\n",
        "\n",
        "def preprocess_dataset_final(examples, tokenizer, truncation, max_length, padding, config):\n",
        "  basic_tokenizer_data = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=truncation, max_length=max_length, padding=padding)\n",
        "  final_tokeniser = FinalTokeniser(tokeniser=tokenizer, config=config)\n",
        "  enriched_data = final_tokeniser.tokenise_everything(examples[\"sentence1\"], examples[\"sentence2\"], truncation=truncation, max_length=max_length, padding=padding)\n",
        "  assert(basic_tokenizer_data['input_ids'] == enriched_data['input_ids'])\n",
        "  for feature, value in enriched_data.items():\n",
        "    basic_tokenizer_data[feature] = value\n",
        "  return basic_tokenizer_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd8lJ5jDt-z0",
        "outputId": "69d215a5-ac94-4102-b6fa-3a997b22971a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = load_dataset(\"glue\", 'mrpc', split=f\"train{DATASET_PART}\")\n",
        "dataset_eval = load_dataset(\"glue\", 'mrpc', split=f\"validation{DATASET_PART}\")\n",
        "dataset_test = load_dataset(\"glue\", 'mrpc', split=f\"test{DATASET_PART}\")\n",
        "\n",
        "config = {\n",
        "    # 'att_dep_tree_value': 0.9,\n",
        "    # 'att_dep_tree_pad_value': 0.,\n",
        "    # 'tokeniser_list': ['attention_dep'],\n",
        "    # baseline\n",
        "    'tokeniser_list': ['attention_one'],\n",
        "}\n",
        "\n",
        "preprocess_dataset_with_full_v2 = partial(\n",
        "    preprocess_dataset_final,\n",
        "    tokenizer=model_tokenizer,\n",
        "    truncation=TRUNCATION,\n",
        "    max_length=MAX_LEN,\n",
        "    padding=PADDING,\n",
        "    config=config,\n",
        "  )\n",
        "\n",
        "collator = DataCollatorWithPadding(model_tokenizer)\n",
        "\n",
        "def prepare_dataloader(dataset, collator):\n",
        "  dataset = dataset.map(preprocess_dataset_with_full_v2, batched=False)\n",
        "  dataset = dataset.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "  dataset = dataset.rename_column(\"label\", \"labels\")\n",
        "  dataset.set_format(\"torch\")\n",
        "  dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collator)\n",
        "  return dataloader\n",
        "\n",
        "\n",
        "train_dataloader = prepare_dataloader(dataset_train, collator)\n",
        "eval_dataloader = prepare_dataloader(dataset_eval, collator)\n",
        "test_dataloader = prepare_dataloader(dataset_test, collator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "ebd0444cbecb41afb393ef627e2264b2",
            "b1f63ea47d5d43acaa1312f5d5e12ad6",
            "ad66208133c042e1b59e5dfd10a2ac57",
            "6d3fbd9bbb3a41779a45b2e36c6bf80a",
            "a742602989f249a882eb66d3e33ba30e",
            "b91bbeb0c5bd4a02ad6bd7cce8de9e43",
            "9adea17bed5f46d68ee60e955e6cf710",
            "c3db4f1a307f48b481fcd66689f2400f",
            "30f4087352ef4bcd9b3b75769dfd0e66",
            "7c511f75bf2049fa8a5fca54a7315441",
            "688cbcc926c64059bf152a696a264b00",
            "4a1deb5c054c48d889b4305013481a84",
            "56ac8c0be67d455ea1973f765cc58abd",
            "278627f37492431da0cf042d789360eb",
            "899cd9f7b01f40fdaedbb1226ef04868",
            "3f285d931ba649e3ba7d2c164230a165",
            "f2eb477e507e4015ac4b10591e29380f",
            "8a936920ce3540299173f29b9d7d8f4e",
            "5b18b4b797db4e4da8f7849a6f0f35b9",
            "99377d19c29b455bb46b927cbaa12f0f",
            "dc77b33e5c4544bcb900111465fa9fd9",
            "bb7a0faeea864c93b17d414a54f1f872",
            "3d7d20ea7b7a42df8be025927dee4d2a",
            "8fb62469e2c64c0ba749d713a17a8c9c",
            "f97d55e78dd0436a9d552cd6c6e53dd2",
            "25e0f61ecf1e4b6d8734d779527fc45a",
            "53ff24871b8c4423803238e3bca1bc15",
            "e9e3e599b7f144c29f29b1eb9382aba0",
            "53af2a863e294b97921b2dcb226e0775",
            "daccd22f0de8412594bcd0722469f0f3",
            "6ee5219eb94c4fcd9cb3a7e505e8974c",
            "99f9f7e36b3d444c8149011fc16761a1",
            "7ee5b924b23243c4a5746f88103044e6"
          ]
        },
        "id": "9VyY6A5ViFln",
        "outputId": "be9d63c7-9c5b-4276-d97f-9bc37725cfd2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dill/_dill.py:412: PicklingWarning: Cannot locate reference to <enum 'TokeniserType'>.\n",
            "  StockPickler.save(self, obj, save_persistent_id)\n",
            "/usr/local/lib/python3.10/dist-packages/dill/_dill.py:412: PicklingWarning: Cannot pickle <enum 'TokeniserType'>: __main__.TokeniserType has recursive self-references that trigger a RecursionError.\n",
            "  StockPickler.save(self, obj, save_persistent_id)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3668 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebd0444cbecb41afb393ef627e2264b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a1deb5c054c48d889b4305013481a84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d7d20ea7b7a42df8be025927dee4d2a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in train_dataloader:\n",
        "  print(b.keys())\n",
        "  break\n",
        "for b in eval_dataloader:\n",
        "  print(b.keys())\n",
        "  break\n",
        "for b in test_dataloader:\n",
        "  print(b.keys())\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJgOeNznYN2D",
        "outputId": "a7d53aba-070c-459c-f04c-18f52eaf00fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'attention_enhencer'])\n",
            "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'attention_enhencer'])\n",
            "dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'attention_enhencer'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBLt54T69QH1",
        "outputId": "41c71e53-314e-45a3-ac35-2900a003ef41"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tokeniser_list': ['attention_one']}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "num_training_steps = NUM_TRAIN_EPOCHS * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "UbHm-r_NfqlA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_eval_test(model, optimizer, lr_scheduler,  train_dataloader, eval_dataloader,test_dataloader,  num_train_epochs, num_training_steps, device):\n",
        "  progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "  def prepare_batch(b):\n",
        "    if 'attention_enhencer' in b.keys():\n",
        "      (B, L, L) = b['attention_enhencer'].size()\n",
        "      number_of_attention_heads = 12\n",
        "      b['attention_enhencer'] = b['attention_enhencer'][:, None, :, :].expand([B, number_of_attention_heads, L, L])\n",
        "    return b\n",
        "  for epoch in range(num_train_epochs):\n",
        "      print(f\"Epoch {epoch}\")\n",
        "      model.train()\n",
        "      for batch in train_dataloader:\n",
        "          batch = prepare_batch(batch)\n",
        "          batch = {k: v.to(device) for k, v in batch.items()}\n",
        "          outputs = model(**batch)\n",
        "          loss = outputs.loss\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "          lr_scheduler.step()\n",
        "          optimizer.zero_grad()\n",
        "          progress_bar.update(1)\n",
        "\n",
        "      accuracy_metric = evaluate.load(\"accuracy\")\n",
        "      f1_metric = evaluate.load(\"f1\")\n",
        "      model.eval()\n",
        "      for batch in eval_dataloader:\n",
        "          batch = prepare_batch(batch)\n",
        "          batch = {k: v.to(device) for k, v in batch.items()}\n",
        "          with torch.no_grad():\n",
        "              outputs = model(**batch)\n",
        "\n",
        "          logits = outputs.logits\n",
        "          predictions = torch.argmax(logits, dim=-1)\n",
        "          accuracy_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "          f1_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "      acc = accuracy_metric.compute()\n",
        "      f1 = f1_metric.compute()\n",
        "      print(f\"Eval accuracy {acc['accuracy']:.4f}\")\n",
        "      print(f\"Eval F1 {f1['f1']:.4f}\")\n",
        "\n",
        "\n",
        "      test_accuracy_metric = evaluate.load(\"accuracy\")\n",
        "      test_f1_metric = evaluate.load(\"f1\")\n",
        "      model.eval()\n",
        "      for batch in test_dataloader:\n",
        "          batch = prepare_batch(batch)\n",
        "          batch = {k: v.to(device) for k, v in batch.items()}\n",
        "          with torch.no_grad():\n",
        "              outputs = model(**batch)\n",
        "\n",
        "          logits = outputs.logits\n",
        "          predictions = torch.argmax(logits, dim=-1)\n",
        "          test_accuracy_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "          test_f1_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "      acc = test_accuracy_metric.compute()\n",
        "      f1 = test_f1_metric.compute()\n",
        "      print(f\"Test accuracy {acc['accuracy']:.4f}\")\n",
        "      print(f\"Test F1 {f1['f1']:.4f}\")\n",
        "\n",
        "\n",
        "train_eval_test(\n",
        "  model = model,\n",
        "  optimizer = optimizer,\n",
        "  lr_scheduler = lr_scheduler,\n",
        "  train_dataloader = train_dataloader,\n",
        "  eval_dataloader = eval_dataloader,\n",
        "  test_dataloader = test_dataloader,\n",
        "  num_train_epochs = NUM_TRAIN_EPOCHS,\n",
        "  num_training_steps = num_training_steps,\n",
        "  device = device,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787,
          "referenced_widgets": [
            "d5f65b81e7de4569a8cefd5e7757cd44",
            "c939ee00bb1e494fa65fbe4a531160f1",
            "9afe5f3b8f41451bae9313dafcea6def",
            "2faafa4db800468a9d3c6d8a5b73ea3d",
            "e709c868fd1744499afedd3e11ab8970",
            "f6e786a4bb584b23bde97a1c9da70d16",
            "531ecae32715415c9856f32b0713e020",
            "efbf4203dc364acbbeb57940ee1a2f57",
            "74036d21a6e54a1a88e59748cf2ba855",
            "be05ccc2ac04491ebacfa61e2e2cbcf9",
            "411c4e162ced44779c44ae6599368d0e"
          ]
        },
        "id": "78w9MeOagAja",
        "outputId": "ea6a8779-638f-492f-f213-b1af9d11a361"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4590 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5f65b81e7de4569a8cefd5e7757cd44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Eval accuracy 0.8333\n",
            "Eval F1 0.8885\n",
            "Test accuracy 0.8081\n",
            "Test F1 0.8713\n",
            "Epoch 1\n",
            "Eval accuracy 0.8995\n",
            "Eval F1 0.9287\n",
            "Test accuracy 0.8713\n",
            "Test F1 0.9071\n",
            "Epoch 2\n",
            "Eval accuracy 0.8922\n",
            "Eval F1 0.9225\n",
            "Test accuracy 0.8696\n",
            "Test F1 0.9041\n",
            "Epoch 3\n",
            "Eval accuracy 0.9044\n",
            "Eval F1 0.9319\n",
            "Test accuracy 0.8730\n",
            "Test F1 0.9071\n",
            "Epoch 4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-023e0626f59e>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m train_eval_test(\n\u001b[0m\u001b[1;32m     66\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-023e0626f59e>\u001b[0m in \u001b[0;36mtrain_eval_test\u001b[0;34m(model, optimizer, lr_scheduler, train_dataloader, eval_dataloader, test_dataloader, num_train_epochs, num_training_steps, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m           \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m         \u001b[0;34m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2799\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2800\u001b[0m         \u001b[0mn_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2801\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2793\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: F811\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2794\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2795\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2778\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2780\u001b[0;31m         formatted_output = format_table(\n\u001b[0m\u001b[1;32m   2781\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2782\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mpython_formatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRowFormat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/torch_formatter.py\u001b[0m in \u001b[0;36mformat_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursive_tensorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mextract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arrow_array_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_arrow_array_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_array\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arrow_array_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_arrow_array_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_array\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m_arrow_array_to_numpy\u001b[0;34m(self, pa_array)\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0;32mnot\u001b[0m \u001b[0m_is_array_with_nulls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpa_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 )\n\u001b[0;32m--> 178\u001b[0;31m                 array: List = [\n\u001b[0m\u001b[1;32m    179\u001b[0m                     \u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpa_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunks\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_copy_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_copy_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 )\n\u001b[1;32m    178\u001b[0m                 array: List = [\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mrow\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpa_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunks\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_copy_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_copy_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 ]\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import login\n",
        "\n",
        "# huggingface_token = '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'\n",
        "\n",
        "# login(token=huggingface_token)\n",
        "# m = model_name.split('/')[-1]\n",
        "# model.push_to_hub(f\"VitaliiVrublevskyi/mrpc_{m}_base_dummy_attention\")"
      ],
      "metadata": {
        "id": "1eVp8ufH5rou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline, enhancer 1 to all.\n",
        "# MAX_LEN = 256\n",
        "# BATCH_SIZE = 8\n",
        "# SEED = 42\n",
        "# NUM_TRAIN_EPOCHS = 10\n",
        "\n",
        "# Epoch 0\n",
        "# Eval accuracy 0.8333\n",
        "# Eval F1 0.8885\n",
        "# Test accuracy 0.8081\n",
        "# Test F1 0.8713\n",
        "# Epoch 1\n",
        "# Eval accuracy 0.8995\n",
        "# Eval F1 0.9287\n",
        "# Test accuracy 0.8713\n",
        "# Test F1 0.9071\n",
        "# Epoch 2\n",
        "# Eval accuracy 0.8922\n",
        "# Eval F1 0.9225\n",
        "# Test accuracy 0.8696\n",
        "# Test F1 0.9041\n",
        "# Epoch 3\n",
        "# Eval accuracy 0.9044\n",
        "# Eval F1 0.9319\n",
        "# Test accuracy 0.8730\n",
        "# Test F1 0.9071\n",
        "# Epoch 4\n",
        "# Eval accuracy 0.8873\n",
        "# Eval F1 0.9199\n",
        "# Test accuracy 0.8754\n",
        "# Test F1 0.9098\n",
        "# Epoch 5\n",
        "# Eval accuracy 0.8873\n",
        "# Eval F1 0.9190\n",
        "# Test accuracy 0.8852  <------------------\n",
        "# Test F1 0.9154 <------------------\n",
        "# Epoch 6\n",
        "# Eval accuracy 0.8873\n",
        "# Eval F1 0.9201\n",
        "# Test accuracy 0.8748\n",
        "# Test F1 0.9097\n",
        "# Epoch 7\n",
        "# Eval accuracy 0.8603\n",
        "# Eval F1 0.9052\n",
        "# Test accuracy 0.8330\n",
        "# Test F1 0.8850\n",
        "# Epoch 8\n",
        "# Eval accuracy 0.9044\n",
        "# Eval F1 0.9322\n",
        "# Test accuracy 0.8719\n",
        "# Test F1 0.9072\n",
        "# Epoch 9\n",
        "# Eval accuracy 0.9020\n",
        "# Eval F1 0.9298\n",
        "# Test accuracy 0.8736\n",
        "# Test F1 0.9072"
      ],
      "metadata": {
        "id": "fH0qHkDarfVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dep Enhancer, 1.2 increase, 0 pad\n",
        "# MAX_LEN = 256\n",
        "# BATCH_SIZE = 8\n",
        "# SEED = 42\n",
        "# NUM_TRAIN_EPOCHS = 10\n",
        "# Epoch 0\n",
        "# Eval accuracy 0.6740\n",
        "# Eval F1 0.7899\n",
        "# Test accuracy 0.6910\n",
        "# Test F1 0.7983\n",
        "# Epoch 1\n",
        "# Eval accuracy 0.7010\n",
        "# Eval F1 0.8201\n",
        "# Test accuracy 0.6916\n",
        "# Test F1 0.8104\n",
        "# Epoch 2\n",
        "# Eval accuracy 0.8627\n",
        "# Eval F1 0.9076\n",
        "# Test accuracy 0.8267\n",
        "# Test F1 0.8808\n",
        "# Epoch 3\n",
        "# Eval accuracy 0.8897\n",
        "# Eval F1 0.9220\n",
        "# Test accuracy 0.8545\n",
        "# Test F1 0.8940\n",
        "# Epoch 4\n",
        "# Eval accuracy 0.8701\n",
        "# Eval F1 0.9103\n",
        "# Test accuracy 0.8452\n",
        "# Test F1 0.8907\n",
        "# Epoch 5\n",
        "# Eval accuracy 0.8775\n",
        "# Eval F1 0.9161\n",
        "# Test accuracy 0.8533\n",
        "# Test F1 0.8970\n",
        "# Epoch 6\n",
        "# Eval accuracy 0.8775\n",
        "# Eval F1 0.9161\n",
        "# Test accuracy 0.8516\n",
        "# Test F1 0.8959\n",
        "# Epoch 7\n",
        "# Eval accuracy 0.8873\n",
        "# Eval F1 0.9201\n",
        "# Test accuracy 0.8701\n",
        "# Test F1 0.9047\n",
        "# Epoch 8\n",
        "# Eval accuracy 0.8897\n",
        "# Eval F1 0.9220\n",
        "# Test accuracy 0.8736 <--------------\n",
        "# Test F1 0.9075 <--------------\n",
        "# Epoch 9\n",
        "# Eval accuracy 0.8971\n",
        "# Eval F1 0.9281\n",
        "# Test accuracy 0.8667\n",
        "# Test F1 0.9039"
      ],
      "metadata": {
        "id": "b3FxjLCdMzKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dep Enhancer, 1.1 increase, 0 pad\n",
        "# MAX_LEN = 256\n",
        "# BATCH_SIZE = 8\n",
        "# SEED = 42\n",
        "# NUM_TRAIN_EPOCHS = 10\n",
        "# Epoch 0\n",
        "# Eval accuracy 0.6838\n",
        "# Eval F1 0.8122\n",
        "# Test accuracy 0.6649\n",
        "# Test F1 0.7987\n",
        "# Epoch 1\n",
        "# Eval accuracy 0.6961\n",
        "# Eval F1 0.8182\n",
        "# Test accuracy 0.6829\n",
        "# Test F1 0.8072\n",
        "# Epoch 2\n",
        "# Eval accuracy 0.8750\n",
        "# Eval F1 0.9140\n",
        "# Test accuracy 0.8487\n",
        "# Test F1 0.8933\n",
        "# Epoch 3\n",
        "# Eval accuracy 0.8725\n",
        "# Eval F1 0.9044\n",
        "# Test accuracy 0.8475\n",
        "# Test F1 0.8818\n",
        "# Epoch 4\n",
        "# Eval accuracy 0.8627\n",
        "# Eval F1 0.9060\n",
        "# Test accuracy 0.8464\n",
        "# Test F1 0.8928\n",
        "# Epoch 5\n",
        "# Eval accuracy 0.8848\n",
        "# Eval F1 0.9180\n",
        "# Test accuracy 0.8684\n",
        "# Test F1 0.9039\n",
        "# Epoch 6\n",
        "# Eval accuracy 0.8750\n",
        "# Eval F1 0.9113\n",
        "# Test accuracy 0.8678\n",
        "# Test F1 0.9029\n",
        "# Epoch 7\n",
        "# Eval accuracy 0.8799\n",
        "# Eval F1 0.9139\n",
        "# Test accuracy 0.8707\n",
        "# Test F1 0.9032\n",
        "# Epoch 8\n",
        "# Eval accuracy 0.8897\n",
        "# Eval F1 0.9212\n",
        "# Test accuracy 0.8771   <--------------\n",
        "# Test F1 0.9089         <--------------\n",
        "# Epoch 9\n",
        "# Eval accuracy 0.8824\n",
        "# Eval F1 0.9172\n",
        "# Test accuracy 0.8719\n",
        "# Test F1 0.9062"
      ],
      "metadata": {
        "id": "wrs5jFe5mvZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dep Enhancer, 0.9 increase, 0 pad\n",
        "# MAX_LEN = 256\n",
        "# BATCH_SIZE = 8\n",
        "# SEED = 42\n",
        "# NUM_TRAIN_EPOCHS = 10\n",
        "# Epoch 0\n",
        "# Eval accuracy 0.8701\n",
        "# Eval F1 0.9115\n",
        "# Test accuracy 0.8359\n",
        "# Test F1 0.8874\n",
        "# Epoch 1\n",
        "# Eval accuracy 0.8873\n",
        "# Eval F1 0.9210\n",
        "# Test accuracy 0.8742\n",
        "# Test F1 0.9097\n",
        "# Epoch 2\n",
        "# Eval accuracy 0.8824\n",
        "# Eval F1 0.9134\n",
        "# Test accuracy 0.8835\n",
        "# Test F1 0.9127\n",
        "# Epoch 3\n",
        "# Eval accuracy 0.8848\n",
        "# Eval F1 0.9180\n",
        "# Test accuracy 0.8730\n",
        "# Test F1 0.9081\n",
        "# Epoch 4\n",
        "# Eval accuracy 0.9020\n",
        "# Eval F1 0.9306\n",
        "# Test accuracy 0.8794\n",
        "# Test F1 0.9116\n",
        "# Epoch 5\n",
        "# Eval accuracy 0.9167\n",
        "# Eval F1 0.9401\n",
        "# Test accuracy 0.8846    <--------------\n",
        "# Test F1 0.9148          <--------------\n",
        "# Epoch 6\n",
        "# Eval accuracy 0.8995\n",
        "# Eval F1 0.9292\n",
        "# Test accuracy 0.8788\n",
        "# Test F1 0.9130\n",
        "# Epoch 7\n",
        "# Eval accuracy 0.9020\n",
        "# Eval F1 0.9306\n",
        "# Test accuracy 0.8800\n",
        "# Test F1 0.9130\n",
        "# Epoch 8\n",
        "# Eval accuracy 0.9044\n",
        "# Eval F1 0.9319\n",
        "# Test accuracy 0.8817\n",
        "# Test F1 0.9139\n",
        "# Epoch 9\n",
        "# Eval accuracy 0.9044\n",
        "# Eval F1 0.9319\n",
        "# Test accuracy 0.8817\n",
        "# Test F1 0.9141"
      ],
      "metadata": {
        "id": "WpvnQeok8rUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dep Enhancer, 0.8 increase, 0 pad\n",
        "# MAX_LEN = 256\n",
        "# BATCH_SIZE = 8\n",
        "# SEED = 42\n",
        "# NUM_TRAIN_EPOCHS = 10\n",
        "# Epoch 0\n",
        "# Eval accuracy 0.8848\n",
        "# Eval F1 0.9191\n",
        "# Test accuracy 0.8516\n",
        "# Test F1 0.8947\n",
        "# Epoch 1\n",
        "# Eval accuracy 0.8873\n",
        "# Eval F1 0.9207\n",
        "# Test accuracy 0.8603\n",
        "# Test F1 0.9000\n",
        "# Epoch 2\n",
        "# Eval accuracy 0.9020\n",
        "# Eval F1 0.9293\n",
        "# Test accuracy 0.8754\n",
        "# Test F1 0.9075\n",
        "# Epoch 3\n",
        "# Eval accuracy 0.9044\n",
        "# Eval F1 0.9315\n",
        "# Test accuracy 0.8643\n",
        "# Test F1 0.9019\n",
        "# Epoch 4\n",
        "# Eval accuracy 0.8946\n",
        "# Eval F1 0.9247\n",
        "# Test accuracy 0.8701\n",
        "# Test F1 0.9064\n",
        "# Epoch 5\n",
        "# Eval accuracy 0.8848\n",
        "# Eval F1 0.9156\n",
        "# Test accuracy 0.8771\n",
        "# Test F1 0.9076\n",
        "# Epoch 6\n",
        "# Eval accuracy 0.8922\n",
        "# Eval F1 0.9211\n",
        "# Test accuracy 0.8748\n",
        "# Test F1 0.9064\n",
        "# Epoch 7\n",
        "# Eval accuracy 0.8995\n",
        "# Eval F1 0.9277\n",
        "# Test accuracy 0.8777\n",
        "# Test F1 0.9106\n",
        "# Epoch 8\n",
        "# Eval accuracy 0.8995\n",
        "# Eval F1 0.9277\n",
        "# Test accuracy 0.8852\n",
        "# Test F1 0.9157\n",
        "# Epoch 9\n",
        "# Eval accuracy 0.8995\n",
        "# Eval F1 0.9279\n",
        "# Test accuracy 0.8823\n",
        "# Test F1 0.9146"
      ],
      "metadata": {
        "id": "M-dod-3cR6U9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}